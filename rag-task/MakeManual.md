# ğŸš€ OpenAI Agents SDK Ã— PostgreSQL pgvector RAGã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰ å®Œå…¨ãƒãƒ³ã‚ºã‚ªãƒ³ã‚¬ã‚¤ãƒ‰

## ğŸ“‹ ã¯ã˜ã‚ã«

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Macç’°å¢ƒã§OpenAI Agents SDKã¨PostgreSQL pgvectorã‚’ä½¿ç”¨ã—ãŸRAGã‚·ã‚¹ãƒ†ãƒ ã‚’ã€ã‚¼ãƒ­ã‹ã‚‰æ®µéšçš„ã«æ§‹ç¯‰ã—ã¦ã„ãã¾ã™ã€‚å„ã‚¹ãƒ†ãƒƒãƒ—ã§å‹•ä½œç¢ºèªã‚’è¡Œã„ãªãŒã‚‰ã€ç¢ºå®Ÿã«ç†è§£ã‚’æ·±ã‚ã¦ã„ãã¾ã—ã‚‡ã†ã€‚

### ã“ã®ã‚¬ã‚¤ãƒ‰ã®ç‰¹å¾´
- âœ… åˆå¿ƒè€…ã§ã‚‚ç†è§£ã§ãã‚‹è©³ç´°ãªèª¬æ˜
- âœ… å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®å‹•ä½œç¢ºèª
- âœ… ã‚¨ãƒ©ãƒ¼å¯¾å‡¦æ³•ã®è¨˜è¼‰
- âœ… æ®µéšçš„ãªå®Ÿè£…ã«ã‚ˆã‚‹ç€å®Ÿãªå­¦ç¿’

---

## ğŸ›  Step 0: é–‹ç™ºç’°å¢ƒã®æº–å‚™

### 0.1 å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã®ç¢ºèªã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

#### Homebrewã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæ—¢ã«ã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
```bash
# HomebrewãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
brew --version

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆ
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

#### uvã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
# uvã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl -LsSf https://astral.sh/uv/install.sh | sh

# ãƒ‘ã‚¹ã‚’é€šã™ï¼ˆ.zshrcã¾ãŸã¯.bash_profileã«è¿½åŠ ï¼‰
echo 'export PATH="$HOME/.cargo/bin:$PATH"' >> ~/.zshrc
source ~/.zshrc

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
uv --version
# å‡ºåŠ›ä¾‹: uv 0.4.18
```

#### Podman Desktopã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
# Podmanã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
brew install podman

# Podman Desktopã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
brew install --cask podman-desktop

# Podmanãƒã‚·ãƒ³ã®åˆæœŸåŒ–
podman machine init
podman machine start

# å‹•ä½œç¢ºèª
podman --version
# å‡ºåŠ›ä¾‹: podman version 4.9.3
```

### 0.2 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ

```bash
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
mkdir ~/rag-agents-project
cd ~/rag-agents-project

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã‚’ä½œæˆ
mkdir -p src/{models,services,tools,utils}
mkdir -p tests
mkdir -p data
mkdir -p config

# åŸºæœ¬çš„ãªè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
touch .env
touch .gitignore
touch README.md
```

### 0.3 .gitignoreã®è¨­å®š

```bash
cat << 'EOF' > .gitignore
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
.venv/

# uv
.python-version

# ç’°å¢ƒå¤‰æ•°
.env
.env.*

# ãƒ‡ãƒ¼ã‚¿
data/*.txt
data/*.pdf
data/*.json

# ãƒ­ã‚°
logs/
*.log

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db
EOF
```

---

## ğŸ³ Step 1: PostgreSQL + pgvectorã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1.1 PostgreSQL + pgvectorã‚³ãƒ³ãƒ†ãƒŠã®ä½œæˆ

#### Containerfileã®ä½œæˆ
```bash
# Containerfileã‚’ä½œæˆï¼ˆDockerfileã®Podmanç‰ˆï¼‰
cat << 'EOF' > Containerfile
FROM docker.io/postgres:16.4

# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
RUN apt-get update && apt-get install -y \
    postgresql-server-dev-16 \
    git \
    make \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# pgvectorã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
RUN cd /tmp && \
    git clone --branch v0.8.0 https://github.com/pgvector/pgvector.git && \
    cd pgvector && \
    make && \
    make install && \
    cd / && \
    rm -rf /tmp/pgvector

# å¾Œå‡¦ç†
RUN apt-get remove -y git make gcc g++ && \
    apt-get autoremove -y && \
    apt-get clean
EOF

echo "âœ… Containerfileã‚’ä½œæˆã—ã¾ã—ãŸ"
```

### 1.2 ã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ¡ãƒ¼ã‚¸ã®ãƒ“ãƒ«ãƒ‰ã¨èµ·å‹•

```bash
# ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ãƒ“ãƒ«ãƒ‰
podman build -t postgres-pgvector -f Containerfile .

# ãƒ“ãƒ«ãƒ‰æˆåŠŸã®ç¢ºèª
podman images | grep postgres-pgvector

# PostgreSQLã‚³ãƒ³ãƒ†ãƒŠã‚’èµ·å‹•
podman run -d \
  --name ragdb \
  -e POSTGRES_PASSWORD=ragpassword \
  -e POSTGRES_USER=raguser \
  -e POSTGRES_DB=ragdatabase \
  -p 5432:5432 \
  -v ragdb_data:/var/lib/postgresql/data \
  postgres-pgvector

# ã‚³ãƒ³ãƒ†ãƒŠã®èµ·å‹•ç¢ºèª
podman ps
# CONTAINER ID  IMAGE                COMMAND     CREATED         STATUS         PORTS                   NAMES
# xxxxx         postgres-pgvector    postgres    10 seconds ago  Up 10 seconds  0.0.0.0:5432->5432/tcp  ragdb

echo "âœ… PostgreSQLã‚³ãƒ³ãƒ†ãƒŠãŒèµ·å‹•ã—ã¾ã—ãŸ"
```

### 1.3 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸè¨­å®š

```bash
# PostgreSQLã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
brew install postgresql@16

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶šã—ã¦pgvectoræ‹¡å¼µã‚’æœ‰åŠ¹åŒ–
PGPASSWORD=ragpassword psql -h localhost -U raguser -d ragdatabase << 'EOF'
-- pgvectoræ‹¡å¼µã®ç¢ºèªã¨æœ‰åŠ¹åŒ–
CREATE EXTENSION IF NOT EXISTS vector;

-- æ‹¡å¼µãŒæœ‰åŠ¹ã«ãªã£ãŸã‹ç¢ºèª
SELECT * FROM pg_extension WHERE extname = 'vector';

-- ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ
CREATE TABLE IF NOT EXISTS test_vectors (
    id SERIAL PRIMARY KEY,
    embedding vector(3)
);

-- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æŒ¿å…¥
INSERT INTO test_vectors (embedding) VALUES
    ('[1,2,3]'),
    ('[4,5,6]');

-- å‹•ä½œç¢ºèªï¼ˆé¡ä¼¼åº¦æ¤œç´¢ã®ãƒ†ã‚¹ãƒˆï¼‰
SELECT id, embedding, embedding <-> '[1,2,3]' as distance
FROM test_vectors
ORDER BY embedding <-> '[1,2,3]'
LIMIT 5;
EOF

echo "âœ… pgvectorã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸ"
```

### 1.4 å‹•ä½œç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
# å‹•ä½œç¢ºèªç”¨ã®Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ
cat << 'EOF' > test_db_connection.py
#!/usr/bin/env python3
import psycopg2

try:
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶š
    conn = psycopg2.connect(
        host="localhost",
        database="ragdatabase",
        user="raguser",
        password="ragpassword",
        port="5432"
    )
    
    cursor = conn.cursor()
    cursor.execute("SELECT version();")
    db_version = cursor.fetchone()
    print(f"âœ… PostgreSQLæ¥ç¶šæˆåŠŸï¼")
    print(f"   ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {db_version[0]}")
    
    # pgvectorã®ç¢ºèª
    cursor.execute("SELECT extversion FROM pg_extension WHERE extname = 'vector';")
    vector_version = cursor.fetchone()
    print(f"âœ… pgvectoræœ‰åŠ¹ï¼")
    print(f"   ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {vector_version[0]}")
    
    cursor.close()
    conn.close()
    
except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
EOF

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
python3 test_db_connection.py
```

---

## ğŸ Step 2: Pythonç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 2.1 uvã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆæœŸåŒ–

```bash
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆæœŸåŒ–
uv init --python 3.11

# Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ç¢ºèª
uv python list
uv python pin 3.11

# pyproject.tomlã®ç¢ºèª
cat pyproject.toml
```

### 2.2 å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# åŸºæœ¬çš„ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
uv add openai
uv add asyncpg
uv add pgvector
uv add sqlalchemy
uv add python-dotenv
uv add numpy
uv add tiktoken

# é–‹ç™ºç”¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
uv add --dev pytest
uv add --dev pytest-asyncio
uv add --dev ipython

echo "âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸ"
```

### 2.3 ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

```bash
# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat << 'EOF' > .env
# Database Configuration
DATABASE_URL=postgresql://raguser:ragpassword@localhost:5432/ragdatabase
ASYNC_DATABASE_URL=postgresql+asyncpg://raguser:ragpassword@localhost:5432/ragdatabase

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Application Configuration
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSION=1536
CHUNK_SIZE=512
CHUNK_OVERLAP=50
EOF

echo "âš ï¸  .envãƒ•ã‚¡ã‚¤ãƒ«ã®OPENAI_API_KEYã‚’å®Ÿéš›ã®ã‚­ãƒ¼ã«ç½®ãæ›ãˆã¦ãã ã•ã„"
```

### 2.4 è¨­å®šãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ

```bash
# è¨­å®šãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
cat << 'EOF' > src/utils/config.py
"""è¨­å®šç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"""
import os
from dotenv import load_dotenv
from typing import Optional

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

class Config:
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š"""
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
    DATABASE_URL: str = os.getenv('DATABASE_URL', '')
    ASYNC_DATABASE_URL: str = os.getenv('ASYNC_DATABASE_URL', '')
    
    # OpenAIè¨­å®š
    OPENAI_API_KEY: str = os.getenv('OPENAI_API_KEY', '')
    EMBEDDING_MODEL: str = os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')
    EMBEDDING_DIMENSION: int = int(os.getenv('EMBEDDING_DIMENSION', '1536'))
    
    # ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°è¨­å®š
    CHUNK_SIZE: int = int(os.getenv('CHUNK_SIZE', '512'))
    CHUNK_OVERLAP: int = int(os.getenv('CHUNK_OVERLAP', '50'))
    
    @classmethod
    def validate(cls) -> bool:
        """è¨­å®šã®æ¤œè¨¼"""
        if not cls.DATABASE_URL:
            print("âŒ DATABASE_URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        if not cls.OPENAI_API_KEY or cls.OPENAI_API_KEY == 'your_openai_api_key_here':
            print("âŒ OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        print("âœ… è¨­å®šã®æ¤œè¨¼ãŒå®Œäº†ã—ã¾ã—ãŸ")
        return True

if __name__ == "__main__":
    # è¨­å®šã®ç¢ºèª
    Config.validate()
    print(f"DATABASE_URL: {Config.DATABASE_URL[:30]}...")
    print(f"EMBEDDING_MODEL: {Config.EMBEDDING_MODEL}")
    print(f"CHUNK_SIZE: {Config.CHUNK_SIZE}")
EOF

# å‹•ä½œç¢ºèª
uv run python src/utils/config.py
```

---

## ğŸ“Š Step 3: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…

### 3.1 SQLAlchemyãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ

```bash
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
cat << 'EOF' > src/models/database.py
"""ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©"""
from datetime import datetime
from typing import Optional, Dict, Any
import json

from sqlalchemy import (
    create_engine, Column, Integer, Text, 
    DateTime, JSON, Float, Index
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from pgvector.sqlalchemy import Vector
from sqlalchemy.ext.asyncio import (
    create_async_engine, AsyncSession, async_sessionmaker
)

from src.utils.config import Config

# ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹
Base = declarative_base()

class Document(Base):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«"""
    __tablename__ = "documents"
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    content = Column(Text, nullable=False)
    embedding = Column(Vector(Config.EMBEDDING_DIMENSION))
    metadata = Column(JSON, default={})
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å®šç¾©
    __table_args__ = (
        Index(
            'idx_embedding_hnsw',
            'embedding',
            postgresql_using='hnsw',
            postgresql_with={'m': 16, 'ef_construction': 64},
            postgresql_ops={'embedding': 'vector_cosine_ops'}
        ),
    )
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            'id': self.id,
            'content': self.content,
            'metadata': self.metadata,
            'created_at': self.created_at.isoformat() if self.created_at else None
        }

# éåŒæœŸã‚¨ãƒ³ã‚¸ãƒ³ã®ä½œæˆ
async_engine = create_async_engine(
    Config.ASYNC_DATABASE_URL,
    pool_pre_ping=True,
    pool_size=10,
    max_overflow=20,
    echo=False  # SQLãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹å ´åˆã¯True
)

# éåŒæœŸã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¯ãƒˆãƒª
AsyncSessionLocal = async_sessionmaker(
    async_engine,
    class_=AsyncSession,
    expire_on_commit=False
)

# åŒæœŸã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆç”¨ï¼‰
sync_engine = create_engine(
    Config.DATABASE_URL,
    pool_pre_ping=True,
    echo=False
)

async def init_database():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆï¼ˆåŒæœŸçš„ã«å®Ÿè¡Œï¼‰
    Base.metadata.create_all(bind=sync_engine)
    print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ")

async def get_session() -> AsyncSession:
    """éåŒæœŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å–å¾—"""
    async with AsyncSessionLocal() as session:
        yield session

if __name__ == "__main__":
    # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã®ãƒ†ã‚¹ãƒˆ
    import asyncio
    asyncio.run(init_database())
EOF

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–
uv run python -c "import asyncio; from src.models.database import init_database; asyncio.run(init_database())"
```

### 3.2 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã®ãƒ†ã‚¹ãƒˆ

```bash
# æ¥ç¶šãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ
cat << 'EOF' > test_database.py
"""ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã®ãƒ†ã‚¹ãƒˆ"""
import asyncio
from src.models.database import AsyncSessionLocal, Document
from sqlalchemy import select, text

async def test_connection():
    """æ¥ç¶šãƒ†ã‚¹ãƒˆ"""
    try:
        async with AsyncSessionLocal() as session:
            # pgvectorã®å‹•ä½œç¢ºèª
            result = await session.execute(
                text("SELECT extversion FROM pg_extension WHERE extname = 'vector'")
            )
            version = result.scalar()
            print(f"âœ… pgvectorãƒãƒ¼ã‚¸ãƒ§ãƒ³: {version}")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã®ç¢ºèª
            result = await session.execute(
                text("SELECT COUNT(*) FROM documents")
            )
            count = result.scalar()
            print(f"âœ… documentsãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {count}")
            
            # ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æŒ¿å…¥
            test_doc = Document(
                content="ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™",
                metadata={"test": True}
            )
            session.add(test_doc)
            await session.commit()
            print("âœ… ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æŒ¿å…¥ã—ã¾ã—ãŸ")
            
            # æ¤œç´¢ãƒ†ã‚¹ãƒˆ
            stmt = select(Document).limit(1)
            result = await session.execute(stmt)
            doc = result.scalar_one_or_none()
            if doc:
                print(f"âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—æˆåŠŸ: {doc.content[:30]}...")
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(test_connection())
EOF

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
uv run python test_database.py
```

---

## ğŸ§® Step 4: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã®å®Ÿè£…

### 4.1 ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã®ä½œæˆ

```bash
# ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆ
cat << 'EOF' > src/services/embedding_service.py
"""ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹"""
import asyncio
from typing import List, Optional
import numpy as np
from openai import AsyncOpenAI
import tiktoken

from src.utils.config import Config

class EmbeddingService:
    """OpenAIã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.client = AsyncOpenAI(api_key=Config.OPENAI_API_KEY)
        self.model = Config.EMBEDDING_MODEL
        self.dimension = Config.EMBEDDING_DIMENSION
        self.encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
        print(f"âœ… EmbeddingServiceåˆæœŸåŒ–å®Œäº† (ãƒ¢ãƒ‡ãƒ«: {self.model})")
    
    async def generate_embedding(self, text: str) -> np.ndarray:
        """å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ"""
        if not text:
            raise ValueError("ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
        text = text.replace("\n", " ").strip()
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®ç¢ºèª
        token_count = len(self.encoding.encode(text))
        if token_count > 8192:  # OpenAIã®åˆ¶é™
            print(f"âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã¾ã™ ({token_count} tokens)ã€‚åˆ‡ã‚Šè©°ã‚ã¾ã™ã€‚")
            tokens = self.encoding.encode(text)[:8192]
            text = self.encoding.decode(tokens)
        
        try:
            # OpenAI APIã‚’å‘¼ã³å‡ºã—
            response = await self.client.embeddings.create(
                input=text,
                model=self.model
            )
            
            embedding = response.data[0].embedding
            return np.array(embedding, dtype=np.float32)
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def generate_batch_embeddings(
        self, 
        texts: List[str],
        batch_size: int = 10
    ) -> List[np.ndarray]:
        """ãƒãƒƒãƒã§ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ"""
        embeddings = []
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
        processed_texts = [
            text.replace("\n", " ").strip() 
            for text in texts if text
        ]
        
        # ãƒãƒƒãƒå‡¦ç†
        for i in range(0, len(processed_texts), batch_size):
            batch = processed_texts[i:i + batch_size]
            
            try:
                response = await self.client.embeddings.create(
                    input=batch,
                    model=self.model
                )
                
                batch_embeddings = [
                    np.array(data.embedding, dtype=np.float32) 
                    for data in response.data
                ]
                embeddings.extend(batch_embeddings)
                
                print(f"âœ… ãƒãƒƒãƒ {i//batch_size + 1} å®Œäº† ({len(batch)} texts)")
                
            except Exception as e:
                print(f"âŒ ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                raise
        
        return embeddings
    
    def estimate_cost(self, text_count: int, avg_tokens: int = 100) -> float:
        """ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Šï¼ˆå‚è€ƒå€¤ï¼‰"""
        # text-embedding-3-small: $0.020 / 1M tokens
        total_tokens = text_count * avg_tokens
        cost = (total_tokens / 1_000_000) * 0.020
        return cost

# ãƒ†ã‚¹ãƒˆé–¢æ•°
async def test_embedding_service():
    """ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ†ã‚¹ãƒˆ"""
    service = EmbeddingService()
    
    # å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ†ã‚¹ãƒˆ
    test_text = "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚RAGã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ã‚’å­¦ã‚“ã§ã„ã¾ã™ã€‚"
    embedding = await service.generate_embedding(test_text)
    print(f"âœ… ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”ŸæˆæˆåŠŸ")
    print(f"   æ¬¡å…ƒæ•°: {len(embedding)}")
    print(f"   æœ€åˆã®5è¦ç´ : {embedding[:5]}")
    
    # ãƒãƒƒãƒå‡¦ç†ã®ãƒ†ã‚¹ãƒˆ
    test_texts = [
        "OpenAI Agents SDKã¯å¼·åŠ›ã§ã™",
        "PostgreSQLã¨pgvectorã®çµ„ã¿åˆã‚ã›",
        "RAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰æ–¹æ³•"
    ]
    embeddings = await service.generate_batch_embeddings(test_texts)
    print(f"âœ… ãƒãƒƒãƒã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”ŸæˆæˆåŠŸ")
    print(f"   ç”Ÿæˆæ•°: {len(embeddings)}")
    
    # ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Š
    cost = service.estimate_cost(100, 50)
    print(f"ğŸ’° 100ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå¹³å‡50ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã®ã‚³ã‚¹ãƒˆè¦‹ç©: ${cost:.4f}")

if __name__ == "__main__":
    asyncio.run(test_embedding_service())
EOF

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆOpenAI APIã‚­ãƒ¼ãŒå¿…è¦ï¼‰
uv run python src/services/embedding_service.py
```

### 4.2 ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚«ãƒ¼ã®å®Ÿè£…

```bash
# ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚«ãƒ¼ã‚’ä½œæˆ
cat << 'EOF' > src/services/text_chunker.py
"""ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ã‚µãƒ¼ãƒ“ã‚¹"""
from typing import List, Dict, Any
import tiktoken
import re

from src.utils.config import Config

class TextChunker:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
    
    def __init__(
        self,
        chunk_size: int = None,
        overlap: int = None
    ):
        """åˆæœŸåŒ–"""
        self.chunk_size = chunk_size or Config.CHUNK_SIZE
        self.overlap = overlap or Config.CHUNK_OVERLAP
        self.encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
        
    def split_text(self, text: str) -> List[Dict[str, Any]]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ãƒ™ãƒ¼ã‚¹ã§åˆ†å‰²"""
        if not text:
            return []
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokens = self.encoding.encode(text)
        total_tokens = len(tokens)
        
        chunks = []
        start = 0
        chunk_index = 0
        
        while start < total_tokens:
            # ãƒãƒ£ãƒ³ã‚¯ã®çµ‚äº†ä½ç½®ã‚’è¨ˆç®—
            end = min(start + self.chunk_size, total_tokens)
            
            # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è€ƒæ…®ã—ãŸé–‹å§‹ä½ç½®
            if start > 0:
                actual_start = max(0, start - self.overlap)
            else:
                actual_start = start
            
            # ãƒãƒ£ãƒ³ã‚¯ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
            chunk_tokens = tokens[actual_start:end]
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«æˆ»ã™
            chunk_text = self.encoding.decode(chunk_tokens)
            
            chunks.append({
                'content': chunk_text,
                'metadata': {
                    'chunk_index': chunk_index,
                    'start_token': actual_start,
                    'end_token': end,
                    'token_count': len(chunk_tokens),
                    'total_chunks': None  # å¾Œã§æ›´æ–°
                }
            })
            
            chunk_index += 1
            
            # æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã®é–‹å§‹ä½ç½®ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è€ƒæ…®ï¼‰
            if end < total_tokens:
                start = end - self.overlap
            else:
                start = end
        
        # total_chunksã‚’æ›´æ–°
        for chunk in chunks:
            chunk['metadata']['total_chunks'] = len(chunks)
        
        return chunks
    
    def split_by_sentences(
        self, 
        text: str,
        max_chunk_size: int = 1000,
        min_chunk_size: int = 100
    ) -> List[Dict[str, Any]]:
        """æ–‡ç« å˜ä½ã§ã®åˆ†å‰²ï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãªå¢ƒç•Œã‚’ä¿æŒï¼‰"""
        # æ–‡ç« ã®å¢ƒç•Œã§åˆ†å‰²
        sentence_pattern = r'(?<=[ã€‚ï¼ï¼Ÿ\.!?])\s+'
        sentences = re.split(sentence_pattern, text)
        
        chunks = []
        current_chunk = []
        current_size = 0
        chunk_index = 0
        
        for sentence in sentences:
            if not sentence.strip():
                continue
                
            sentence_tokens = len(self.encoding.encode(sentence))
            
            # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ ã™ã‚‹ã¨ä¸Šé™ã‚’è¶…ãˆã‚‹å ´åˆ
            if current_size + sentence_tokens > max_chunk_size and current_chunk:
                # ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
                chunk_text = ' '.join(current_chunk)
                chunks.append({
                    'content': chunk_text,
                    'metadata': {
                        'chunk_index': chunk_index,
                        'token_count': current_size,
                        'sentence_count': len(current_chunk),
                        'type': 'sentence_based'
                    }
                })
                
                chunk_index += 1
                current_chunk = [sentence]
                current_size = sentence_tokens
            else:
                current_chunk.append(sentence)
                current_size += sentence_tokens
        
        # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯
        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            if current_size >= min_chunk_size:
                chunks.append({
                    'content': chunk_text,
                    'metadata': {
                        'chunk_index': chunk_index,
                        'token_count': current_size,
                        'sentence_count': len(current_chunk),
                        'type': 'sentence_based'
                    }
                })
            elif chunks:
                # å°ã•ã™ãã‚‹å ´åˆã¯å‰ã®ãƒãƒ£ãƒ³ã‚¯ã«çµåˆ
                chunks[-1]['content'] += ' ' + chunk_text
                chunks[-1]['metadata']['token_count'] += current_size
                chunks[-1]['metadata']['sentence_count'] += len(current_chunk)
        
        return chunks

def test_chunker():
    """ãƒãƒ£ãƒ³ã‚«ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    chunker = TextChunker(chunk_size=100, overlap=20)
    
    # ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
    test_text = """
    OpenAI Agents SDKã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®AIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ§‹ç¯‰ã®ãŸã‚ã®
    é©æ–°çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ã“ã®SDKã¯ã€è¤‡æ•°ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå”èª¿ã—ã¦
    è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹ç’°å¢ƒã‚’æä¾›ã—ã¾ã™ã€‚
    
    å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç‹¬è‡ªã®å½¹å‰²ã¨èƒ½åŠ›ã‚’æŒã¡ã€å¿…è¦ã«å¿œã˜ã¦ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«
    ã‚¿ã‚¹ã‚¯ã‚’å§”è­²ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é«˜åº¦ã«ç‰¹åŒ–ã—ãŸ
    ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
    
    ã¾ãŸã€pgvectorã‚’ä½¿ç”¨ã—ãŸãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã®çµ±åˆã«ã‚ˆã‚Šã€
    åŠ¹ç‡çš„ãªé¡ä¼¼åº¦æ¤œç´¢ãŒå¯èƒ½ã«ãªã‚Šã€RAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ãŒå®¹æ˜“ã«ãªã‚Šã¾ã™ã€‚
    """
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ™ãƒ¼ã‚¹ã®åˆ†å‰²
    chunks = chunker.split_text(test_text)
    print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒ³ãƒ™ãƒ¼ã‚¹åˆ†å‰²: {len(chunks)} ãƒãƒ£ãƒ³ã‚¯")
    for i, chunk in enumerate(chunks):
        print(f"\nãƒãƒ£ãƒ³ã‚¯ {i + 1}:")
        print(f"  å†…å®¹: {chunk['content'][:50]}...")
        print(f"  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {chunk['metadata']}")
    
    # æ–‡ç« ãƒ™ãƒ¼ã‚¹ã®åˆ†å‰²
    chunks_sentence = chunker.split_by_sentences(test_text, max_chunk_size=150)
    print(f"\nâœ… æ–‡ç« ãƒ™ãƒ¼ã‚¹åˆ†å‰²: {len(chunks_sentence)} ãƒãƒ£ãƒ³ã‚¯")
    for i, chunk in enumerate(chunks_sentence):
        print(f"\nãƒãƒ£ãƒ³ã‚¯ {i + 1}:")
        print(f"  å†…å®¹: {chunk['content'][:50]}...")
        print(f"  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {chunk['metadata']}")

if __name__ == "__main__":
    test_chunker()
EOF

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
uv run python src/services/text_chunker.py
```

---

## ğŸ—„ï¸ Step 5: ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®å®Ÿè£…

### 5.1 ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ

```bash
# ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
cat << 'EOF' > src/services/vector_store.py
"""ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®å®Ÿè£…"""
import asyncio
from typing import List, Optional, Dict, Any
from contextlib import asynccontextmanager
import numpy as np
import asyncpg
from pgvector.asyncpg import register_vector

from src.models.database import AsyncSessionLocal, Document
from src.services.embedding_service import EmbeddingService
from src.services.text_chunker import TextChunker
from src.utils.config import Config
from sqlalchemy import select, text

class VectorStore:
    """pgvectorã‚’ä½¿ç”¨ã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.pool = None
        self.embedding_service = EmbeddingService()
        self.chunker = TextChunker()
        
    async def initialize(self):
        """ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ«ã®åˆæœŸåŒ–"""
        self.pool = await asyncpg.create_pool(
            Config.DATABASE_URL,
            min_size=5,
            max_size=20,
            init=self._init_connection
        )
        print("âœ… VectorStoreã®ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    
    async def _init_connection(self, conn):
        """pgvectoræ‹¡å¼µã®ç™»éŒ²"""
        await register_vector(conn)
    
    async def close(self):
        """ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ«ã‚’é–‰ã˜ã‚‹"""
        if self.pool:
            await self.pool.close()
            print("âœ… ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ«ã‚’é–‰ã˜ã¾ã—ãŸ")
    
    async def add_document(
        self,
        content: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[int]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ """
        if not content:
            raise ValueError("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒç©ºã§ã™")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        chunks = self.chunker.split_text(content)
        print(f"ğŸ“„ {len(chunks)} ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸ")
        
        # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç”Ÿæˆ
        chunk_texts = [chunk['content'] for chunk in chunks]
        embeddings = await self.embedding_service.generate_batch_embeddings(
            chunk_texts
        )
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        document_ids = []
        async with AsyncSessionLocal() as session:
            for chunk, embedding in zip(chunks, embeddings):
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®çµåˆ
                chunk_metadata = chunk['metadata']
                if metadata:
                    chunk_metadata.update(metadata)
                
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ
                doc = Document(
                    content=chunk['content'],
                    embedding=embedding.tolist(),
                    metadata=chunk_metadata
                )
                session.add(doc)
                await session.flush()
                document_ids.append(doc.id)
            
            await session.commit()
            print(f"âœ… {len(document_ids)} ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        return document_ids
    
    async def similarity_search(
        self,
        query: str,
        k: int = 5,
        threshold: float = 0.7
    ) -> List[Dict[str, Any]]:
        """é¡ä¼¼åº¦æ¤œç´¢"""
        # ã‚¯ã‚¨ãƒªã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
        query_embedding = await self.embedding_service.generate_embedding(query)
        
        async with self.pool.acquire() as conn:
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã«ã‚ˆã‚‹æ¤œç´¢
            results = await conn.fetch(
                """
                SELECT 
                    id,
                    content,
                    metadata,
                    1 - (embedding <=> $1::vector) as similarity
                FROM documents
                WHERE 1 - (embedding <=> $1::vector) > $2
                ORDER BY embedding <=> $1::vector
                LIMIT $3
                """,
                query_embedding.tolist(),
                threshold,
                k
            )
            
            # çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            search_results = []
            for row in results:
                search_results.append({
                    'id': row['id'],
                    'content': row['content'],
                    'metadata': row['metadata'],
                    'similarity': float(row['similarity'])
                })
            
            print(f"ğŸ” {len(search_results)} ä»¶ã®çµæœã‚’å–å¾—ã—ã¾ã—ãŸ")
            return search_results
    
    async def hybrid_search(
        self,
        query: str,
        keyword: Optional[str] = None,
        k: int = 5,
        vector_weight: float = 0.7
    ) -> List[Dict[str, Any]]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆãƒ™ã‚¯ãƒˆãƒ« + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰"""
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®æº–å‚™
        query_embedding = await self.embedding_service.generate_embedding(query)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨
        search_keyword = keyword or query
        
        async with self.pool.acquire() as conn:
            results = await conn.fetch(
                """
                SELECT 
                    id,
                    content,
                    metadata,
                    1 - (embedding <=> $1::vector) as vector_similarity,
                    ts_rank(
                        to_tsvector('english', content),
                        plainto_tsquery('english', $2)
                    ) as keyword_score
                FROM documents
                WHERE 
                    to_tsvector('english', content) @@ plainto_tsquery('english', $2)
                    OR 1 - (embedding <=> $1::vector) > 0.5
                ORDER BY 
                    (1 - (embedding <=> $1::vector)) * $3 + 
                    ts_rank(
                        to_tsvector('english', content),
                        plainto_tsquery('english', $2)
                    ) * (1 - $3) DESC
                LIMIT $4
                """,
                query_embedding.tolist(),
                search_keyword,
                vector_weight,
                k
            )
            
            return [dict(row) for row in results]
    
    async def delete_document(self, document_id: int) -> bool:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‰Šé™¤"""
        async with AsyncSessionLocal() as session:
            result = await session.execute(
                select(Document).where(Document.id == document_id)
            )
            doc = result.scalar_one_or_none()
            
            if doc:
                await session.delete(doc)
                await session.commit()
                print(f"âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {document_id} ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                return True
            
            print(f"âš ï¸ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {document_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
    
    async def get_statistics(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±ã®å–å¾—"""
        async with self.pool.acquire() as conn:
            # ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            count_result = await conn.fetchval(
                "SELECT COUNT(*) FROM documents"
            )
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚º
            index_result = await conn.fetchval(
                """
                SELECT pg_size_pretty(
                    pg_relation_size('idx_embedding_hnsw')
                )
                """
            )
            
            # å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«
            avg_result = await conn.fetchval(
                "SELECT COUNT(DISTINCT metadata->>'chunk_index') FROM documents"
            )
            
            return {
                'total_documents': count_result,
                'index_size': index_result,
                'unique_chunks': avg_result
            }

# ãƒ†ã‚¹ãƒˆé–¢æ•°
async def test_vector_store():
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ãƒ†ã‚¹ãƒˆ"""
    store = VectorStore()
    
    try:
        # åˆæœŸåŒ–
        await store.initialize()
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¿½åŠ 
        test_content = """
        RAGï¼ˆRetrieval-Augmented Generationï¼‰ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’
        å¤–éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã§æ‹¡å¼µã™ã‚‹æŠ€è¡“ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ˆã‚Šæ­£ç¢ºã§æœ€æ–°ã®
        æƒ…å ±ã«åŸºã¥ã„ãŸå›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
        
        pgvectorã¯ã€PostgreSQLã§ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦æ¤œç´¢ã‚’å¯èƒ½ã«ã™ã‚‹æ‹¡å¼µæ©Ÿèƒ½ã§ã€
        é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ãªä¿å­˜ã¨æ¤œç´¢ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
        """
        
        doc_ids = await store.add_document(
            test_content,
            metadata={'source': 'test', 'type': 'tutorial'}
        )
        print(f"âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID: {doc_ids}")
        
        # é¡ä¼¼åº¦æ¤œç´¢
        search_results = await store.similarity_search(
            "RAGã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            k=3
        )
        
        print("\nğŸ” æ¤œç´¢çµæœ:")
        for i, result in enumerate(search_results):
            print(f"\nçµæœ {i + 1}:")
            print(f"  é¡ä¼¼åº¦: {result['similarity']:.4f}")
            print(f"  å†…å®¹: {result['content'][:100]}...")
        
        # çµ±è¨ˆæƒ…å ±
        stats = await store.get_statistics()
        print(f"\nğŸ“Š çµ±è¨ˆæƒ…å ±: {stats}")
        
    finally:
        await store.close()

if __name__ == "__main__":
    asyncio.run(test_vector_store())
EOF

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
uv run python src/services/vector_store.py
```

---

## ğŸ¤– Step 6: OpenAI Agents SDKã®çµ±åˆ

### 6.1 OpenAI Agents SDKã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ç¢ºèª

```bash
# OpenAI Agents SDKã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
uv add openai-agents

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
uv run python -c "import agents; print(f'OpenAI Agents SDK version: {agents.__version__ if hasattr(agents, '__version__') else 'unknown'}')"
```

### 6.2 RAGãƒ„ãƒ¼ãƒ«ã®å®Ÿè£…

```bash
# RAGãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆ
cat << 'EOF' > src/tools/rag_tools.py
"""RAGç”¨ã®ãƒ„ãƒ¼ãƒ«å®Ÿè£…"""
import asyncio
from typing import Optional, Dict, Any
from dataclasses import dataclass
from agents import function_tool, RunContextWrapper

from src.services.vector_store import VectorStore
from src.services.embedding_service import EmbeddingService
from src.services.text_chunker import TextChunker

@dataclass
class RAGContext:
    """RAGã‚·ã‚¹ãƒ†ãƒ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"""
    vector_store: VectorStore
    max_results: int = 5
    similarity_threshold: float = 0.7

@function_tool
async def search_knowledge_base(
    ctx: RunContextWrapper[RAGContext],
    query: str,
    num_results: Optional[int] = None
) -> str:
    """
    çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢ã—ã¾ã™ã€‚
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        num_results: å–å¾—ã™ã‚‹çµæœæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰
    
    Returns:
        æ¤œç´¢çµæœã®æ•´å½¢ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    try:
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
        k = num_results or ctx.context.max_results
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®å®Ÿè¡Œ
        results = await ctx.context.vector_store.similarity_search(
            query=query,
            k=k,
            threshold=ctx.context.similarity_threshold
        )
        
        if not results:
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # çµæœã®æ•´å½¢
        response_parts = []
        response_parts.append(f"ğŸ” ã€Œ{query}ã€ã«é–¢ã™ã‚‹æ¤œç´¢çµæœ:\n")
        
        for idx, result in enumerate(results, 1):
            similarity_percent = result['similarity'] * 100
            response_parts.append(
                f"\nã€çµæœ {idx}ã€‘(é–¢é€£åº¦: {similarity_percent:.1f}%)\n"
                f"{result['content']}\n"
            )
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
            if result.get('metadata'):
                meta = result['metadata']
                if 'source' in meta:
                    response_parts.append(f"å‡ºå…¸: {meta['source']}\n")
        
        return "\n".join(response_parts)
        
    except Exception as e:
        error_msg = f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        print(f"âŒ {error_msg}")
        return error_msg

@function_tool
async def add_to_knowledge_base(
    ctx: RunContextWrapper[RAGContext],
    content: str,
    source: Optional[str] = None,
    category: Optional[str] = None
) -> str:
    """
    æ–°ã—ã„æƒ…å ±ã‚’çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ ã—ã¾ã™ã€‚
    
    Args:
        content: è¿½åŠ ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        source: æƒ…å ±æºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        category: ã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    
    Returns:
        è¿½åŠ çµæœã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    try:
        if not content or len(content.strip()) < 10:
            return "âŒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒçŸ­ã™ãã¾ã™ã€‚ã‚‚ã£ã¨è©³ç´°ãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        metadata = {}
        if source:
            metadata['source'] = source
        if category:
            metadata['category'] = category
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¿½åŠ 
        doc_ids = await ctx.context.vector_store.add_document(
            content=content,
            metadata=metadata
        )
        
        success_msg = (
            f"âœ… çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã¸ã®è¿½åŠ ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n"
            f"   - è¿½åŠ ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æ•°: {len(doc_ids)}\n"
            f"   - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID: {doc_ids[0] if doc_ids else 'N/A'}"
        )
        
        if source:
            success_msg += f"\n   - ã‚½ãƒ¼ã‚¹: {source}"
        if category:
            success_msg += f"\n   - ã‚«ãƒ†ã‚´ãƒªãƒ¼: {category}"
        
        return success_msg
        
    except Exception as e:
        error_msg = f"è¿½åŠ ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        print(f"âŒ {error_msg}")
        return error_msg

@function_tool
async def analyze_knowledge_base(
    ctx: RunContextWrapper[RAGContext]
) -> str:
    """
    çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚
    
    Returns:
        çµ±è¨ˆæƒ…å ±ã®æ•´å½¢ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    try:
        stats = await ctx.context.vector_store.get_statistics()
        
        response = (
            "ğŸ“Š çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®çµ±è¨ˆæƒ…å ±:\n"
            f"   - ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {stats.get('total_documents', 0)}\n"
            f"   - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚º: {stats.get('index_size', 'N/A')}\n"
            f"   - ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒãƒ£ãƒ³ã‚¯æ•°: {stats.get('unique_chunks', 0)}"
        )
        
        return response
        
    except Exception as e:
        error_msg = f"çµ±è¨ˆæƒ…å ±ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        print(f"âŒ {error_msg}")
        return error_msg

# ãƒ†ã‚¹ãƒˆé–¢æ•°
async def test_rag_tools():
    """RAGãƒ„ãƒ¼ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–
    vector_store = VectorStore()
    await vector_store.initialize()
    
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä½œæˆ
    context = RAGContext(
        vector_store=vector_store,
        max_results=3,
        similarity_threshold=0.6
    )
    
    # ãƒ¢ãƒƒã‚¯ã®RunContextWrapper
    class MockRunContext:
        def __init__(self, ctx):
            self.context = ctx
    
    mock_ctx = MockRunContext(context)
    
    try:
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã¸ã®è¿½åŠ ãƒ†ã‚¹ãƒˆ
        print("ğŸ“ çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã¸ã®è¿½åŠ ãƒ†ã‚¹ãƒˆ...")
        result = await add_to_knowledge_base(
            mock_ctx,
            content="OpenAI Agents SDKã¯ã€ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®è»½é‡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚",
            source="å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ",
            category="æŠ€è¡“"
        )
        print(result)
        
        # æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        print("\nğŸ” æ¤œç´¢ãƒ†ã‚¹ãƒˆ...")
        result = await search_knowledge_base(
            mock_ctx,
            query="OpenAI Agents SDK",
            num_results=2
        )
        print(result)
        
        # çµ±è¨ˆæƒ…å ±ãƒ†ã‚¹ãƒˆ
        print("\nğŸ“Š çµ±è¨ˆæƒ…å ±ãƒ†ã‚¹ãƒˆ...")
        result = await analyze_knowledge_base(mock_ctx)
        print(result)
        
    finally:
        await vector_store.close()

if __name__ == "__main__":
    asyncio.run(test_rag_tools())
EOF

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
uv run python src/tools/rag_tools.py
```

### 6.3 RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…

```bash
# RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ
cat << 'EOF' > src/agents/rag_agent.py
"""RAGå¯¾å¿œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…"""
import asyncio
from typing import Optional, Dict, Any
from agents import Agent, Runner, ModelSettings
from dataclasses import dataclass

from src.tools.rag_tools import (
    RAGContext,
    search_knowledge_base,
    add_to_knowledge_base,
    analyze_knowledge_base
)
from src.services.vector_store import VectorStore

class RAGAgent:
    """RAGæ©Ÿèƒ½ã‚’æŒã¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.vector_store = None
        self.agent = None
        self.context = None
        
    async def initialize(self):
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–"""
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–
        self.vector_store = VectorStore()
        await self.vector_store.initialize()
        
        # RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä½œæˆ
        self.context = RAGContext(
            vector_store=self.vector_store,
            max_results=5,
            similarity_threshold=0.7
        )
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆ
        self.agent = self._create_agent()
        
        print("âœ… RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    
    def _create_agent(self) -> Agent:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆ"""
        
        instructions = """
        ã‚ãªãŸã¯çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ´»ç”¨ã—ã¦è³ªå•ã«ç­”ãˆã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        
        ## ã‚ãªãŸã®å½¹å‰²
        1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢ã—ã¦å›ç­”ã™ã‚‹
        2. æ–°ã—ã„æƒ…å ±ã‚’çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ ã™ã‚‹
        3. å¿…è¦ã«å¿œã˜ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®çµ±è¨ˆæƒ…å ±ã‚’æä¾›ã™ã‚‹
        
        ## å›ç­”ã®åŸå‰‡
        - ã¾ãšçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã¦ã€é–¢é€£æƒ…å ±ãŒã‚ã‚‹ã‹ç¢ºèªã™ã‚‹
        - æ¤œç´¢çµæœã«åŸºã¥ã„ã¦ã€æ­£ç¢ºã§è©³ç´°ãªå›ç­”ã‚’æä¾›ã™ã‚‹
        - æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãã®æ—¨ã‚’æ˜ç¢ºã«ä¼ãˆã‚‹
        - å›ç­”ã«ã¯å¸¸ã«æ ¹æ‹ ã¨ãªã‚‹æƒ…å ±æºã‚’ç¤ºã™
        
        ## ä½¿ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«
        - search_knowledge_base: çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’æ¤œç´¢
        - add_to_knowledge_base: æ–°ã—ã„æƒ…å ±ã‚’è¿½åŠ 
        - analyze_knowledge_base: çµ±è¨ˆæƒ…å ±ã®å–å¾—
        """
        
        agent = Agent[RAGContext](
            name="RAG Assistant",
            instructions=instructions,
            tools=[
                search_knowledge_base,
                add_to_knowledge_base,
                analyze_knowledge_base
            ],
            model="gpt-4o-mini",  # ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®è‰¯ã„ãƒ¢ãƒ‡ãƒ«
            model_settings=ModelSettings(
                temperature=0.3,  # ã‚ˆã‚Šç¢ºå®Ÿãªå›ç­”ã®ãŸã‚ä½ã‚ã«è¨­å®š
                max_tokens=1000
            )
        )
        
        return agent
    
    async def process(self, user_input: str) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å‡¦ç†"""
        try:
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ
            result = await Runner.run(
                self.agent,
                user_input,
                context=self.context
            )
            
            return result.final_output
            
        except Exception as e:
            error_msg = f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            print(f"âŒ {error_msg}")
            return error_msg
    
    async def add_knowledge(self, content: str, source: str = None) -> str:
        """çŸ¥è­˜ã‚’ç›´æ¥è¿½åŠ """
        try:
            doc_ids = await self.vector_store.add_document(
                content=content,
                metadata={'source': source} if source else {}
            )
            return f"âœ… {len(doc_ids)} ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ ã—ã¾ã—ãŸ"
        except Exception as e:
            return f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    async def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.vector_store:
            await self.vector_store.close()
        print("âœ… RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ã‚ºã—ã¾ã—ãŸ")

# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ†ã‚¹ãƒˆ
async def interactive_test():
    """å¯¾è©±å‹ãƒ†ã‚¹ãƒˆ"""
    agent = RAGAgent()
    
    try:
        # åˆæœŸåŒ–
        await agent.initialize()
        
        # åˆæœŸãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
        print("ğŸ“š åˆæœŸãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ä¸­...")
        initial_data = [
            {
                "content": "OpenAI Agents SDKã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®AIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®è»½é‡ã§å¼·åŠ›ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ä¸»ãªç‰¹å¾´ã¨ã—ã¦ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆæŒ‡ç¤ºã¨ãƒ„ãƒ¼ãƒ«ã‚’å‚™ãˆãŸLLMï¼‰ã€ãƒãƒ³ãƒ‰ã‚ªãƒ•ï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®ã‚¿ã‚¹ã‚¯å§”è­²ï¼‰ã€ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ï¼ˆå…¥å‡ºåŠ›ã®æ¤œè¨¼ï¼‰ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆä¼šè©±å±¥æ­´ã®è‡ªå‹•ç®¡ç†ï¼‰ãŒã‚ã‚Šã¾ã™ã€‚",
                "source": "OpenAIå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ"
            },
            {
                "content": "pgvectorã¯ã€PostgreSQLã§ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦æ¤œç´¢ã‚’å¯èƒ½ã«ã™ã‚‹æ‹¡å¼µæ©Ÿèƒ½ã§ã™ã€‚HNSWã¨IVFFlatã®2ã¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ–¹å¼ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ãªä¿å­˜ã¨æ¤œç´¢ã‚’å®Ÿç¾ã—ã¾ã™ã€‚RAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã«ãŠã„ã¦é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚",
                "source": "pgvectoræŠ€è¡“è³‡æ–™"
            },
            {
                "content": "RAGï¼ˆRetrieval-Augmented Generationï¼‰ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ã‚’å¤–éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã§å¼·åŒ–ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ã¾ãšé–¢é€£ã™ã‚‹æƒ…å ±ã‚’æ¤œç´¢ã—ã€ãã®æƒ…å ±ã‚’åŸºã«LLMãŒå›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ˆã‚Šæ­£ç¢ºã§æœ€æ–°ã®æƒ…å ±ã«åŸºã¥ã„ãŸå¿œç­”ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚",
                "source": "AIæŠ€è¡“è§£èª¬"
            }
        ]
        
        for data in initial_data:
            await agent.add_knowledge(data["content"], data["source"])
        
        print("\nâœ… åˆæœŸãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("=" * 60)
        
        # å¯¾è©±ãƒ«ãƒ¼ãƒ—
        print("\nğŸ¤– RAGã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãŒèµ·å‹•ã—ã¾ã—ãŸï¼")
        print("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆçµ‚äº†: 'exit' ã¾ãŸã¯ 'quit'ï¼‰")
        print("=" * 60)
        
        while True:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
            user_input = input("\nğŸ‘¤ You: ").strip()
            
            # çµ‚äº†ã‚³ãƒãƒ³ãƒ‰
            if user_input.lower() in ['exit', 'quit', 'çµ‚äº†']:
                print("ğŸ‘‹ çµ‚äº†ã—ã¾ã™...")
                break
            
            if not user_input:
                continue
            
            # å‡¦ç†ä¸­ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            print("ğŸ¤” è€ƒãˆä¸­...")
            
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å¿œç­”
            response = await agent.process(user_input)
            print(f"\nğŸ¤– Assistant: {response}")
        
    finally:
        await agent.close()

if __name__ == "__main__":
    asyncio.run(interactive_test())
EOF

# å¯¾è©±å‹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
echo "ğŸš€ RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’èµ·å‹•ã—ã¾ã™..."
uv run python src/agents/rag_agent.py
```

---

## ğŸ¯ Step 7: å®Œå…¨ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æ§‹ç¯‰

### 7.1 ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ä½œæˆ

```bash
# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
cat << 'EOF' > main.py
"""RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""
import asyncio
import sys
from typing import Optional
from pathlib import Path

from src.agents.rag_agent import RAGAgent
from src.utils.config import Config

class RAGApplication:
    """RAGã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.agent = None
        
    async def setup(self):
        """ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print("ğŸš€ RAGã‚·ã‚¹ãƒ†ãƒ ã‚’èµ·å‹•ä¸­...")
        
        # è¨­å®šã®æ¤œè¨¼
        if not Config.validate():
            print("âŒ è¨­å®šã‚¨ãƒ©ãƒ¼ã§ã™ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            sys.exit(1)
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.agent = RAGAgent()
        await self.agent.initialize()
        
        print("âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ï¼")
    
    async def load_documents(self, file_path: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–‡æ›¸ã‚’èª­ã¿è¾¼ã¿"""
        path = Path(file_path)
        
        if not path.exists():
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return
        
        print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {file_path}")
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ 
            result = await self.agent.add_knowledge(
                content,
                source=path.name
            )
            print(result)
            
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def interactive_mode(self):
        """å¯¾è©±ãƒ¢ãƒ¼ãƒ‰"""
        print("\n" + "=" * 60)
        print("ğŸ’¬ å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™")
        print("ã‚³ãƒãƒ³ãƒ‰:")
        print("  - 'help': ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º")
        print("  - 'stats': çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º")
        print("  - 'load <file>': ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿")
        print("  - 'exit': çµ‚äº†")
        print("=" * 60)
        
        while True:
            try:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
                user_input = input("\nğŸ‘¤ > ").strip()
                
                if not user_input:
                    continue
                
                # ã‚³ãƒãƒ³ãƒ‰å‡¦ç†
                if user_input.lower() == 'exit':
                    print("ğŸ‘‹ çµ‚äº†ã—ã¾ã™...")
                    break
                    
                elif user_input.lower() == 'help':
                    print("""
ğŸ“– ãƒ˜ãƒ«ãƒ—:
  - é€šå¸¸ã®è³ªå•: ãã®ã¾ã¾å…¥åŠ›ã—ã¦ãã ã•ã„
  - æƒ…å ±ã®è¿½åŠ : ã€Œè¿½åŠ :ã€ã§å§‹ã‚ã¦ãã ã•ã„
  - çµ±è¨ˆæƒ…å ±: 'stats'ã¨å…¥åŠ›
  - ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: 'load <ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹>'
  - çµ‚äº†: 'exit'
                    """)
                    
                elif user_input.lower() == 'stats':
                    response = await self.agent.process(
                        "çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®çµ±è¨ˆæƒ…å ±ã‚’æ•™ãˆã¦ãã ã•ã„"
                    )
                    print(f"\n{response}")
                    
                elif user_input.lower().startswith('load '):
                    file_path = user_input[5:].strip()
                    await self.load_documents(file_path)
                    
                elif user_input.startswith('è¿½åŠ :'):
                    content = user_input[3:].strip()
                    response = await self.agent.process(
                        f"æ¬¡ã®æƒ…å ±ã‚’çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ ã—ã¦ãã ã•ã„: {content}"
                    )
                    print(f"\n{response}")
                    
                else:
                    # é€šå¸¸ã®è³ªå•
                    print("ğŸ¤” å‡¦ç†ä¸­...")
                    response = await self.agent.process(user_input)
                    print(f"\nğŸ¤– {response}")
                    
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.agent:
            await self.agent.close()
        print("âœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
    
    async def run(self):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        try:
            await self.setup()
            await self.interactive_mode()
        finally:
            await self.cleanup()

async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    app = RAGApplication()
    await app.run()

if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     RAG System with OpenAI Agents SDK & PostgreSQL       â•‘
â•‘                    Version 1.0.0                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ çµ‚äº†ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)
EOF

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
uv run python main.py
```

### 7.2 ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™

```bash
# ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
cat << 'EOF' > data/sample_knowledge.txt
# RAGã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºã‚¬ã‚¤ãƒ‰

## OpenAI Agents SDKã®åŸºæœ¬æ¦‚å¿µ

OpenAI Agents SDKã¯ã€2024å¹´ã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸé©æ–°çš„ãªãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚
ã“ã®SDKã®ä¸»ãªç‰¹å¾´ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

1. **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆAgentsï¼‰**: LLMã«æŒ‡ç¤ºã¨ãƒ„ãƒ¼ãƒ«ã‚’è£…å‚™ã—ãŸåŸºæœ¬å˜ä½
2. **ãƒ„ãƒ¼ãƒ«ï¼ˆToolsï¼‰**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå®Ÿè¡Œã§ãã‚‹é–¢æ•°ã‚„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
3. **ãƒãƒ³ãƒ‰ã‚ªãƒ•ï¼ˆHandoffsï¼‰**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã§ã®ã‚¿ã‚¹ã‚¯å§”è­²ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 
4. **ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ï¼ˆGuardrailsï¼‰**: å…¥å‡ºåŠ›ã®æ¤œè¨¼ã¨å®‰å…¨æ€§ç¢ºä¿
5. **ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆSessionsï¼‰**: ä¼šè©±å±¥æ­´ã®è‡ªå‹•ç®¡ç†æ©Ÿèƒ½

## pgvectorã®æŠ€è¡“è©³ç´°

pgvectorã¯ã€PostgreSQLã«é«˜åº¦ãªãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—æ©Ÿèƒ½ã‚’è¿½åŠ ã™ã‚‹æ‹¡å¼µæ©Ÿèƒ½ã§ã™ã€‚

### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ–¹å¼

1. **HNSW (Hierarchical Navigable Small Worlds)**
   - é«˜é€Ÿãªè¿‘ä¼¼æœ€è¿‘å‚æ¢ç´¢
   - ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
   - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: mï¼ˆæ¥ç¶šæ•°ï¼‰ã€ef_constructionï¼ˆæ§‹ç¯‰æ™‚ã®å“è³ªï¼‰

2. **IVFFlat (Inverted File Flat)**
   - ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ã®æ¢ç´¢
   - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒè‰¯ã„
   - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: listsï¼ˆã‚¯ãƒ©ã‚¹ã‚¿æ•°ï¼‰

### è·é›¢é–¢æ•°

- L2è·é›¢: ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢
- å†…ç©: ãƒ‰ãƒƒãƒˆç©ã«ã‚ˆã‚‹é¡ä¼¼åº¦
- ã‚³ã‚µã‚¤ãƒ³è·é›¢: è§’åº¦ã«ã‚ˆã‚‹é¡ä¼¼åº¦

## RAGã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥

1. **å›ºå®šã‚µã‚¤ã‚ºãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°**: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§å‡ç­‰åˆ†å‰²
2. **ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°**: æ„å‘³çš„ãªå¢ƒç•Œã§åˆ†å‰²
3. **ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°**: æ–‡è„ˆä¿æŒã®ãŸã‚ã®é‡è¤‡

### ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ

- **text-embedding-3-small**: ã‚³ã‚¹ãƒˆåŠ¹ç‡é‡è¦–ï¼ˆ1536æ¬¡å…ƒï¼‰
- **text-embedding-3-large**: ç²¾åº¦é‡è¦–ï¼ˆ3072æ¬¡å…ƒï¼‰
- **text-embedding-ada-002**: ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ¢ãƒ‡ãƒ«ï¼ˆ1536æ¬¡å…ƒï¼‰

### æ¤œç´¢ç²¾åº¦ã®å‘ä¸Š

1. **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢**: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
2. **ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°**: åˆæœŸæ¤œç´¢çµæœã®å†é †ä½ä»˜ã‘
3. **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**: ã‚«ãƒ†ã‚´ãƒªã‚„ã‚¿ã‚°ã«ã‚ˆã‚‹çµã‚Šè¾¼ã¿
EOF

echo "âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã—ãŸ: data/sample_knowledge.txt"
```

### 7.3 çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ

```bash
# çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ
cat << 'EOF' > test_integration.py
"""çµ±åˆãƒ†ã‚¹ãƒˆ"""
import asyncio
import time
from src.agents.rag_agent import RAGAgent

async def run_integration_test():
    """çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    print("ğŸ§ª çµ±åˆãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™...")
    
    agent = RAGAgent()
    
    try:
        # 1. åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
        print("\n[Test 1] ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–")
        start_time = time.time()
        await agent.initialize()
        init_time = time.time() - start_time
        print(f"âœ… åˆæœŸåŒ–æˆåŠŸ (æ‰€è¦æ™‚é–“: {init_time:.2f}ç§’)")
        
        # 2. ãƒ‡ãƒ¼ã‚¿è¿½åŠ ãƒ†ã‚¹ãƒˆ
        print("\n[Test 2] ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ")
        test_data = [
            "Pythonã¯æ±ç”¨ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚",
            "æ©Ÿæ¢°å­¦ç¿’ã«ã¯scikit-learnãŒã‚ˆãä½¿ã‚ã‚Œã¾ã™ã€‚",
            "Docker/Podmanã¯ã‚³ãƒ³ãƒ†ãƒŠæŠ€è¡“ã§ã™ã€‚"
        ]
        
        for data in test_data:
            result = await agent.add_knowledge(data, source="test")
            print(f"  {result}")
        
        # 3. æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        print("\n[Test 3] æ¤œç´¢æ©Ÿèƒ½")
        queries = [
            "Pythonã«ã¤ã„ã¦æ•™ãˆã¦",
            "ã‚³ãƒ³ãƒ†ãƒŠæŠ€è¡“ã«ã¤ã„ã¦",
            "æ©Ÿæ¢°å­¦ç¿’ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª"
        ]
        
        for query in queries:
            print(f"\n  Query: {query}")
            response = await agent.process(query)
            print(f"  Response: {response[:100]}...")
        
        # 4. çµ±è¨ˆæƒ…å ±ãƒ†ã‚¹ãƒˆ
        print("\n[Test 4] çµ±è¨ˆæƒ…å ±ã®å–å¾—")
        stats_response = await agent.process("çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®çµ±è¨ˆã‚’è¡¨ç¤º")
        print(f"  {stats_response}")
        
        # 5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        print("\n[Test 5] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š")
        start_time = time.time()
        for _ in range(3):
            await agent.process("ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª")
        avg_time = (time.time() - start_time) / 3
        print(f"âœ… å¹³å‡å¿œç­”æ™‚é–“: {avg_time:.2f}ç§’")
        
        print("\nğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
        
    except Exception as e:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        raise
    
    finally:
        await agent.close()

if __name__ == "__main__":
    asyncio.run(run_integration_test())
EOF

# çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
uv run python test_integration.py
```

---

## ğŸ“ å­¦ç¿’ã®ç¢ºèªã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨å¯¾å‡¦æ³•

#### 1. Podmanã‚³ãƒ³ãƒ†ãƒŠãŒèµ·å‹•ã—ãªã„
```bash
# Podmanãƒã‚·ãƒ³ã®çŠ¶æ…‹ç¢ºèª
podman machine list

# åœæ­¢ã—ã¦ã„ã‚‹å ´åˆã¯å†èµ·å‹•
podman machine stop
podman machine start

# ã‚³ãƒ³ãƒ†ãƒŠã®å†èµ·å‹•
podman restart ragdb
```

#### 2. OpenAI APIã‚­ãƒ¼ã‚¨ãƒ©ãƒ¼
```bash
# .envãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
cat .env | grep OPENAI_API_KEY

# APIã‚­ãƒ¼ã®æœ‰åŠ¹æ€§ç¢ºèª
uv run python -c "
from openai import OpenAI
client = OpenAI()
try:
    client.models.list()
    print('âœ… APIã‚­ãƒ¼æœ‰åŠ¹')
except:
    print('âŒ APIã‚­ãƒ¼ç„¡åŠ¹')
"
```

#### 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼
```bash
# PostgreSQLã®æ¥ç¶šç¢ºèª
PGPASSWORD=ragpassword psql -h localhost -U raguser -d ragdatabase -c "SELECT 1;"

# pgvectorã®ç¢ºèª
PGPASSWORD=ragpassword psql -h localhost -U raguser -d ragdatabase -c "SELECT extversion FROM pg_extension WHERE extname = 'vector';"
```

### å‹•ä½œç¢ºèªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```bash
# å®Œå…¨ãªå‹•ä½œç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ
cat << 'EOF' > check_system.py
"""ã‚·ã‚¹ãƒ†ãƒ å‹•ä½œç¢ºèª"""
import asyncio
import subprocess
import sys
from pathlib import Path

def check_command(cmd, name):
    """ã‚³ãƒãƒ³ãƒ‰ã®å­˜åœ¨ç¢ºèª"""
    try:
        subprocess.run(cmd, capture_output=True, shell=True, check=True)
        print(f"âœ… {name}: OK")
        return True
    except:
        print(f"âŒ {name}: NG")
        return False

async def check_system():
    """ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ç¢ºèª"""
    print("ğŸ” ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­ã‚’é–‹å§‹ã—ã¾ã™...\n")
    
    all_ok = True
    
    # 1. åŸºæœ¬ã‚³ãƒãƒ³ãƒ‰
    print("ã€åŸºæœ¬ã‚³ãƒãƒ³ãƒ‰ã€‘")
    all_ok &= check_command("uv --version", "uv")
    all_ok &= check_command("podman --version", "Podman")
    all_ok &= check_command("python3 --version", "Python")
    
    # 2. Podmanã‚³ãƒ³ãƒ†ãƒŠ
    print("\nã€Podmanã‚³ãƒ³ãƒ†ãƒŠã€‘")
    all_ok &= check_command("podman ps | grep ragdb", "PostgreSQLã‚³ãƒ³ãƒ†ãƒŠ")
    
    # 3. Pythonç’°å¢ƒ
    print("\nã€Pythonç’°å¢ƒã€‘")
    try:
        import openai
        print("âœ… openai: OK")
    except:
        print("âŒ openai: NG")
        all_ok = False
    
    try:
        import asyncpg
        print("âœ… asyncpg: OK")
    except:
        print("âŒ asyncpg: NG")
        all_ok = False
    
    try:
        import agents
        print("âœ… openai-agents: OK")
    except:
        print("âŒ openai-agents: NG")
        all_ok = False
    
    # 4. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
    print("\nã€è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã€‘")
    if Path(".env").exists():
        print("âœ… .env: OK")
    else:
        print("âŒ .env: NG")
        all_ok = False
    
    # 5. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    print("\nã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã€‘")
    try:
        from src.utils.config import Config
        if Config.validate():
            print("âœ… è¨­å®šæ¤œè¨¼: OK")
        else:
            print("âŒ è¨­å®šæ¤œè¨¼: NG")
            all_ok = False
    except Exception as e:
        print(f"âŒ è¨­å®šèª­ã¿è¾¼ã¿: NG ({e})")
        all_ok = False
    
    # çµæœ
    print("\n" + "=" * 40)
    if all_ok:
        print("âœ… ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­: ã™ã¹ã¦æ­£å¸¸ã§ã™ï¼")
    else:
        print("âŒ ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­: å•é¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        print("   ä¸Šè¨˜ã®NGã®é …ç›®ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    
    return all_ok

if __name__ == "__main__":
    result = asyncio.run(check_system())
    sys.exit(0 if result else 1)
EOF

# ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­ã‚’å®Ÿè¡Œ
uv run python check_system.py
```

---

## ğŸ‰ å®Œæˆï¼

ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼RAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **æ©Ÿèƒ½æ‹¡å¼µ**
   - PDFãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿æ©Ÿèƒ½è¿½åŠ 
   - Web UIã®å®Ÿè£…ï¼ˆGradio/Streamlitï¼‰
   - ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®æ‹¡å¼µ

2. **æœ€é©åŒ–**
   - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
   - ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã®å®Ÿè£…
   - ãƒãƒƒãƒå‡¦ç†ã®æœ€é©åŒ–

3. **æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹**
   - Docker/Kuberneteså¯¾å¿œ
   - èªè¨¼ãƒ»èªå¯ã®å®Ÿè£…
   - ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚°åé›†

### å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹

- [OpenAI Agents SDK Documentation](https://openai.github.io/openai-agents-python/)
- [pgvector GitHub](https://github.com/pgvector/pgvector)
- [PostgreSQL Documentation](https://www.postgresql.org/docs/)

é ‘å¼µã£ã¦é–‹ç™ºã‚’ç¶šã‘ã¦ãã ã•ã„ï¼ ğŸš€